{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrfiZTKcjnoQHuBvHH4aiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IyadSultan/low-coding-AI/blob/main/03_Prompt_engineering_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Prompting the OpenAI API with Different Techniques\n",
        "**1. Prerequisites**\n",
        "Before diving into prompting techniques, ensure you have:\n",
        "\n",
        "An OpenAI account\n",
        "An API key from OpenAI\n",
        "A basic understanding of Python (or your preferred language)\n",
        "OpenAI’s API library installed. For Python, install it using:\n",
        "\n",
        "pip install openai"
      ],
      "metadata": {
        "id": "dAuAOMel8G2m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2WYsb61x74vc"
      },
      "outputs": [],
      "source": [
        "# ! pip install openai\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Setting Up the API**\n",
        "Here’s how you can initialize the OpenAI API in Python:"
      ],
      "metadata": {
        "id": "Z_qHZXYk8Hx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY= userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "AVJwtfxC8Iii"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1. Zero-Shot Prompting**\n",
        "In zero-shot prompting, the model is given a task without examples. It relies purely on the task description.\n",
        "\n",
        "\n",
        "Best For:\n",
        "\n",
        "Straightforward tasks\n",
        "When no context is required"
      ],
      "metadata": {
        "id": "N-_dZu2f8KHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your prompt\n",
        "prompt = \"Translate the following English text to French: 'How are you today?'\"\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "response=client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "# print(response)\n",
        "# print(\"==========================================\")\n",
        "extracted_content = response.choices[0].message.content\n",
        "print(extracted_content)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpolXkYv8KuO",
        "outputId": "ab9db6bf-3670-4564-e701-e795379326e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'How are you today?' in French is 'Comment ça va aujourd'hui ?'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: using tiktoken package, claculate the number of tokens in the following sentnece \"I go to work every morning at 8 am and get back home at 5 pM\"\n",
        "\n",
        "!pip install tiktoken\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    print(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "sentence = \"I go to work every morning at 8 am and get back home at 5 pM\"\n",
        "encoding_name = \"cl100k_base\"  # Choose the appropriate encoding\n",
        "num_tokens = num_tokens_from_string(sentence, encoding_name)\n",
        "print(f\"The sentence '{sentence}' has {num_tokens} tokens using the {encoding_name} encoding.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqMaEq61KMcP",
        "outputId": "633f607d-e855-4f2e-d50f-6d199099458c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n",
            "[40, 733, 311, 990, 1475, 6693, 520, 220, 23, 1097, 323, 636, 1203, 2162, 520, 220, 20, 281, 44]\n",
            "The sentence 'I go to work every morning at 8 am and get back home at 5 pM' has 19 tokens using the cl100k_base encoding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2. Few-Shot Prompting**\n",
        "Few-shot prompting provides examples to guide the model.\n",
        "\n",
        "Best For:\n",
        "\n",
        "Tasks needing clarity or context\n",
        "More complex use cases"
      ],
      "metadata": {
        "id": "FB1n5kks8LLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Translate the following sentences from English to French:\n",
        "1. Hello, how are you? -> Bonjour, comment ça va?\n",
        "2. What is your name? -> Comment vous appelez-vous?\n",
        "3. I love programming. -> J'aime programmer.\n",
        "4. Have a great day. ->\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response=client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "\n",
        "extracted_content = response.choices[0].message.content\n",
        "print(extracted_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_hyzPkj8L0n",
        "outputId": "3a19d7d8-da95-4147-c9cb-b68e6a08a60c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. Have a great day. -> Passez une excellente journée.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3. Chain-of-Thought Prompting**\n",
        "\n",
        "This technique involves breaking down reasoning processes step-by-step.\n",
        "\n",
        "Best For:\n",
        "\n",
        "Logical reasoning\n",
        "Complex problem-solving tasks"
      ],
      "metadata": {
        "id": "Uj5K57tK8MYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Solve this math problem and explain your reasoning:\n",
        "If a train travels 60 miles in 1 hour, how far will it travel in 3 hours?\n",
        "Step-by-step reasoning:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response=client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "extracted_content = response.choices[0].message.content\n",
        "print(extracted_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnQFaF0X8M6w",
        "outputId": "f7c67112-bcc5-4762-9c5f-c6acf6c46a2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve the problem of how far a train will travel in 3 hours when it travels at a speed of 60 miles in 1 hour, we can follow these steps:\n",
            "\n",
            "1. **Identify the speed of the train**: The problem states that the train travels 60 miles in 1 hour. This means the speed of the train is 60 miles per hour (mph).\n",
            "\n",
            "2. **Determine the time of travel**: We need to find out how far the train will travel in 3 hours.\n",
            "\n",
            "3. **Use the formula for distance**: Distance can be calculated using the formula:\n",
            "   \\[\n",
            "   \\text{Distance} = \\text{Speed} \\times \\text{Time}\n",
            "   \\]\n",
            "   Here, the speed is 60 miles per hour, and the time is 3 hours.\n",
            "\n",
            "4. **Calculate the distance**:\n",
            "   \\[\n",
            "   \\text{Distance} = 60 \\text{ miles/hour} \\times 3 \\text{ hours} = 180 \\text{ miles}\n",
            "   \\]\n",
            "\n",
            "5. **Conclusion**: Therefore, the train will travel 180 miles in 3 hours.\n",
            "\n",
            "Thus, the final answer is that the train will travel **180 miles** in 3 hours.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4. Instruction-Based Prompting**\n",
        "Provide clear, explicit instructions for the task.\n",
        "\n",
        "Best For:\n",
        "\n",
        "Tasks where clarity is essential\n",
        "Ensuring precise outputs"
      ],
      "metadata": {
        "id": "oent-KXv8PFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are a helpful assistant. Rewrite the following sentence to make it more professional:\n",
        "'Can you send me the details ASAP?'\"\"\"\n",
        "\n",
        "\n",
        "response=client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "extracted_content = response.choices[0].message.content\n",
        "print(extracted_content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntsm2HlH8PtC",
        "outputId": "79521962-1419-4c58-d009-deca211bd783"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Could you please provide me with the details at your earliest convenience?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.5. Role-Playing Prompting**\n",
        "Assign a role to the AI for more tailored responses.\n",
        "\n",
        "Best For:\n",
        "\n",
        "Domain-specific tasks\n",
        "When tone and style matter"
      ],
      "metadata": {
        "id": "b8cu5iaG8QPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\" Suggest a 5-day workout plan for a beginner.\"\"\"\n",
        "\n",
        "\n",
        "response=client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a fitness trainer.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "extracted_content = response.choices[0].message.content\n",
        "print(extracted_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-yo06RY8QqA",
        "outputId": "e3389f14-8139-4424-d115-8f5cacb3009f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here’s a simple and effective 5-day workout plan for beginners. This plan incorporates a mix of strength training, cardio, and flexibility exercises to help build a solid foundation for fitness. Ensure you warm up for 5-10 minutes before each workout and cool down afterward.\n",
            "\n",
            "### 5-Day Beginner Workout Plan\n",
            "\n",
            "#### Day 1: Full Body Strength Training\n",
            "**Warm-up:** 5-10 minutes of brisk walking or light jogging.\n",
            "\n",
            "**Exercises:**\n",
            "1. **Bodyweight Squats** - 3 sets of 10-12 reps\n",
            "2. **Push-Ups (knee or regular)** - 3 sets of 5-10 reps\n",
            "3. **Dumbbell Rows** (using a water bottle if needed) - 3 sets of 10 reps each arm\n",
            "4. **Plank** - 3 sets, hold for 20-30 seconds\n",
            "5. **Glute Bridges** - 3 sets of 10-12 reps\n",
            "\n",
            "**Cool Down:** 5-10 minutes of stretching.\n",
            "\n",
            "---\n",
            "\n",
            "#### Day 2: Cardio & Core\n",
            "**Warm-up:** 5-10 minutes of dynamic stretches or light cardio (jumping jacks, high knees).\n",
            "\n",
            "**Exercises:**\n",
            "1. **Brisk Walking or Light Jogging** - 20-30 minutes\n",
            "2. **Core Circuit (2 rounds):**\n",
            "   - Bicycle Crunches - 10-15 reps\n",
            "   - Side Planks - 15-20 seconds each side\n",
            "   - Leg Raises - 10-12 reps\n",
            "\n",
            "**Cool Down:** 5-10 minutes of stretching focusing on the core.\n",
            "\n",
            "---\n",
            "\n",
            "#### Day 3: Upper Body & Flexibility\n",
            "**Warm-up:** 5-10 minutes of arm circles and shoulder rolls.\n",
            "\n",
            "**Exercises:**\n",
            "1. **Dumbbell Shoulder Press** - 3 sets of 10-12 reps\n",
            "2. **Bicep Curls** - 3 sets of 10-12 reps\n",
            "3. **Tricep Dips (using a chair)** - 3 sets of 8-10 reps\n",
            "4. **Lateral Raises** - 3 sets of 10 reps\n",
            "5. **Cool Down:** Spend 10-15 minutes doing full-body stretching or yoga.\n",
            "\n",
            "---\n",
            "\n",
            "#### Day 4: Lower Body & Cardio\n",
            "**Warm-up:** 5-10 minutes of jogging or jumping rope.\n",
            "\n",
            "**Exercises:**\n",
            "1. **Lunges (forward or reverse)** - 3 sets of 10 reps each leg\n",
            "2. **Side Leg Raises** - 3 sets of 10 reps each leg\n",
            "3. **Calf Raises** - 3 sets of 15 reps\n",
            "4. **Step-Ups (on a sturdy bench or step)** - 3 sets of 10 reps each leg\n",
            "5. **Cardio Finisher:** 10-15 minutes of jumping jacks or high knees.\n",
            "\n",
            "**Cool Down:** 5-10 minutes of lower body stretching.\n",
            "\n",
            "---\n",
            "\n",
            "#### Day 5: Active Recovery & Mobility\n",
            "**Activity Suggestions:**\n",
            "- Go for a long walk (30-60 minutes).\n",
            "- Participate in a gentle yoga or stretching class (online or in-person).\n",
            "- Spend 20-30 minutes doing mobility exercises focusing on all major joints (shoulders, hips, knees, and ankles).\n",
            "\n",
            "**Cool Down:** Spend extra time stretching any sore areas.\n",
            "\n",
            "### Important Tips:\n",
            "- **Hydration:** Keep hydrated before, during, and after workouts.\n",
            "- **Rest:** Listen to your body; if you feel overly fatigued, allow for rest days as needed.\n",
            "- **Nutrition:** Consider pairing your workouts with a healthy diet that provides sufficient protein and nutrients.\n",
            "- **Form:** Focus on form and technique over quantity to prevent injuries.\n",
            "\n",
            "Always consult with a healthcare professional before starting any new exercise program, especially if you have any underlying health conditions or concerns. Enjoy your workouts!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6. Few-Shot with Contextual Instructions**\n",
        "\n",
        "Combine few-shot examples with detailed instructions."
      ],
      "metadata": {
        "id": "5tcqPemH8RHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are a language expert. Translate these sentences to Spanish while keeping the tone casual:\n",
        "1. Where are you going? -> ¿Adónde vas?\n",
        "2. I’m just hanging out. -> Solo estoy pasando el rato.\n",
        "3. Let me know when you’re free. ->\"\"\"\n",
        "\n",
        "response=client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    temperature=0.7,\n",
        "    max_tokens=8,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        ")\n",
        "\n",
        "extracted_content = response.choices[0].message.content\n",
        "print(extracted_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC3t8W5U8RzI",
        "outputId": "1cffacff-b995-4e8c-dbc1-e5c7b728367e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claro! Here’s the translation:  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Fine-Tuning Parameters**\n",
        "Experiment with the following parameters in your API call to optimize results:\n",
        "\n",
        "temperature: Controls randomness (e.g., 0.0 for deterministic results, 1.0 for more creative responses).\n",
        "max_tokens: Limits the length of the output.\n",
        "top_p: Nucleus sampling for controlling diversity.\n",
        "frequency_penalty & presence_penalty: Adjust repetition and encourage new topics."
      ],
      "metadata": {
        "id": "vYwihgiv8SGL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UYFxWlBD8TOq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Debugging Prompts**\n",
        "If the output isn’t as expected:\n",
        "\n",
        "Be explicit: Clearly define tasks and provide context.\n",
        "Test variations: Reword the prompt for better clarity.\n",
        "Break down tasks: Simplify or use chain-of-thought techniques.\n",
        "Analyze outputs: Identify patterns in incorrect outputs and adjust."
      ],
      "metadata": {
        "id": "kD9I59G48Tpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Translate the following sentences from English to French:\n",
        "1. Hello, how are you? -> Bonjour, comment ça va?\n",
        "2. What is your name? -> Comment vous appelez-vous?\n",
        "3. I love programming. -> J'aime programmer.\n",
        "4. Have a great day. ->\n",
        "\n",
        "Make the output in json\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response=client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "\n",
        "extracted_content = response.choices[0].message.content\n",
        "\n",
        "\n",
        "response2=client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You a json detector, if the output is not json format you scream: wrong, otherwhise say: OK\"},\n",
        "         {\"role\": \"user\", \"content\": extracted_content}],\n",
        ")\n",
        "\n",
        "\n",
        "extracted_content2 = response2.choices[0].message.content\n",
        "print(extracted_content2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmdjuPo-8UHA",
        "outputId": "b1ee7440-9ad3-4ea0-938a-0d7d1a054b66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "egdjCEwY8Uli"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6e3F3ZTO8WHS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain\n",
        "\n",
        "**1. Introduction**\n",
        "LangChain is a flexible framework for building LLM (Large Language Model) applications. While it supports many model providers (OpenAI, Anthropic, Ollama, etc.), we’ll focus on OpenAI in this tutorial. By the end, you’ll know how to:\n",
        "\n",
        "Set up your environment and integrate LangChain with the OpenAI API.\n",
        "Prompt the model directly and through Prompt Templates.\n",
        "Work with messages (e.g., SystemMessage, HumanMessage, AIMessage) effectively.\n",
        "Parse responses using message parsers or structured output.\n",
        "Check for and handle errors (e.g., malformed model responses)."
      ],
      "metadata": {
        "id": "-gwyvDAx8WnS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xoj4w-lj8W_C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Setup and Environment**\n",
        "\n",
        "2.1. Installation\n",
        "You can install LangChain (and our recommended extras) via pip:"
      ],
      "metadata": {
        "id": "ZVKbuXpC8XZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44U1mJr08Xxa",
        "outputId": "61f2b9d2-0ba3-41cb-9216-7d1929729e96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Collecting langchain-core<0.4.0,>=0.3.29 (from langchain)\n",
            "  Downloading langchain_core-0.3.31-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.59.6)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.8.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
            "Downloading langchain_openai-0.3.1-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.31-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.29\n",
            "    Uninstalling langchain-core-0.3.29:\n",
            "      Successfully uninstalled langchain-core-0.3.29\n",
            "Successfully installed langchain-core-0.3.31 langchain-openai-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2. Environment Variables**\n",
        "OpenAI typically expects OPENAI_API_KEY in your environment. You can set it via a .env file or directly in Python:"
      ],
      "metadata": {
        "id": "fxUll-B88YL7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6_z97xZq8YwS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xQS2ktKo8ZdW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v87q-_pc8aih"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Basic Prompting with LangChain**\n",
        "\n",
        "To illustrate prompting, let’s start by calling a Chat Model directly with minimal overhead.\n",
        "\n",
        "We instantiate a chat model from the langchain_openai package.\n",
        "We provide a list of messages: in this case, a single HumanMessage asking “Hello, how are you?”\n",
        "The invoke method returns an AIMessage object, from which you can extract response.content."
      ],
      "metadata": {
        "id": "AIJaoFBvBXNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Initialize a chat model (using gpt-4o-mini as an example)\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Simple direct invocation\n",
        "response = model.invoke([HumanMessage(content=\"Hello, how are you?\")])\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfv9CLu0BXuO",
        "outputId": "3836999a-80c8-4310-ba01-901c29e75f7d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Prompt Templates**\n",
        "\n",
        "While providing raw strings to the model can work for quick tests, Prompt Templates let us build more consistent or dynamic prompts.\n",
        "\n",
        "LangChain has a powerful concept of ChatPromptTemplate for chat-based LLMs.\n",
        "\n",
        "**4.1. Example: Simple Translation**\n",
        "\n",
        "Suppose you want a single system message instructing the model to translate text into a specific language, and a user message containing text to be translated.\n",
        "Suppose you want a single system message instructing the model to translate text into a specific language, and a user message containing text to be translated."
      ],
      "metadata": {
        "id": "tYaT2kf2BYMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define your system and user message as templates\n",
        "system_template = \"Translate the following text from English to {language}.\"\n",
        "user_template   = \"{text}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_template),\n",
        "        (\"user\", user_template),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Format the prompt\n",
        "formatted_prompt = prompt_template.invoke(\n",
        "    {\"language\": \"Italian\", \"text\": \"Hi! How are you?\"}\n",
        ")\n",
        "\n",
        "print(formatted_prompt.to_messages())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGy34shpBYgW",
        "outputId": "edc4120f-1512-49c0-df32-1cd683215b7d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='Translate the following text from English to Italian.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2. Call the Model\n",
        "Now we feed this formatted prompt back into our chat model:"
      ],
      "metadata": {
        "id": "OuIg3zu1BY5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation = model.invoke(formatted_prompt)\n",
        "print(translation.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p5cCopjBZUe",
        "outputId": "585c57cf-002a-450a-afc4-319a6635ee51"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao! Come stai?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Messages and Their Roles**\n",
        "\n",
        "Modern LLMs typically accept a list of messages in conversation style. LangChain defines these classes:\n",
        "\n",
        "- SystemMessage: special instructions to guide model behavior.\n",
        "- HumanMessage: user or external input.\n",
        "- AIMessage: model’s own response.\n",
        "- ToolMessage: used to pass results of external function calls back to the model (for advanced tool-calling).\n",
        "- AIMessageChunk: chunked tokens for streaming output.\n",
        "\n",
        "\n",
        "**5.1. Example: Setting a System Instruction **"
      ],
      "metadata": {
        "id": "npDuZ0LABZpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a friendly English-to-French translator.\"),\n",
        "    HumanMessage(content=\"Please translate: 'Good morning, world!'\"),\n",
        "]\n",
        "response = model.invoke(messages)\n",
        "print(response.content)\n",
        "# -> \"Bonjour, le monde!\"\n",
        "\n",
        "\"\"\"\n",
        "Pro tip: If your chosen chat model doesn’t explicitly support “system” messages,\n",
        "LangChain automatically adapts the system message into the best-available input parameter.\n",
        "But for OpenAI, system messages work perfectly.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "W3mLEaAIBaHa",
        "outputId": "ead95af9-023e-45fb-c562-ee0285e6d158"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Bonjour, le monde !'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPro tip: If your chosen chat model doesn’t explicitly support “system” messages,\\nLangChain automatically adapts the system message into the best-available input parameter.\\nBut for OpenAI, system messages work perfectly.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Parsing Model Outputs**\n",
        "\n",
        "Sometimes, you want the model to return structured data (e.g., JSON, CSV, a well-defined Pydantic model). While you can try to prompt the model carefully, it can still hallucinate or produce invalid JSON.\n",
        "\n",
        "**6.1. Direct Output Parsing**\n",
        "\n",
        "The simplest approach is “ask the model for JSON, then json.loads it.” But errors can occur if the model returns malformed JSON.\n",
        "\n",
        "**6.2. Using LangChain’s Output Parsers**\n",
        "LangChain offers Output Parser classes (e.g., JSONOutputParser, PydanticOutputParser, OutputFixingParser) that:\n",
        "\n",
        "Provide format instructions for the prompt.\n",
        "Attempt to parse the response reliably.\n",
        "Optionally fix malformed outputs automatically (via an additional LLM call).\n",
        "Example: Creating a JSON parser that expects a single field called translation.\n",
        "\n"
      ],
      "metadata": {
        "id": "TFrlJTnQBagU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "format_instructions = json_parser.get_format_instructions()\n",
        "print( f\"format instructions:{format_instructions}\")\n",
        "\n",
        "system_message = \"You are a translator. Return your answer as valid JSON.\"\n",
        "user_prompt = f\"Please translate to Spanish:\\nHello!\\n\\n{format_instructions}\"\n",
        "\n",
        "# Build the final prompt\n",
        "message = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"user\", user_prompt)]).invoke({})\n",
        "\n",
        "response = model.invoke(message)\n",
        "\n",
        "try:\n",
        "    parsed_output = json_parser.parse(response.content)\n",
        "    print(parsed_output)\n",
        "except Exception as e:\n",
        "    print(\"Parsing error:\", e)\n",
        "\n",
        "\n",
        "\n",
        "# which you can parse successfully.\n",
        "\n",
        "# If the model is large or well-instructed, this typically works fine. If the model occasionally returns invalid JSON, you may look into more advanced parsers like OutputFixingParser.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwTcoLvOBbIP",
        "outputId": "ef3351ab-8b5b-4659-c1b5-5513ba098266"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "format instructions:Return a JSON object.\n",
            "{'translation': '¡Hola!'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Checking for Errors and Handling Malformed Responses**\n",
        "\n",
        "LLMs can produce unexpected output, especially when you need a strict format. Common pitfalls include:\n",
        "\n",
        "Missing or extra fields in a JSON or CSV output.\n",
        "Additional text or disclaimers.\n",
        "Non-JSON strings.\n",
        "\n",
        "**7.1. Retry Strategies**\n",
        "LangChain provides built-in ways to handle partial or malformed responses:\n",
        "\n",
        "OutputFixingParser: Uses an additional LLM call to correct the response if parsing fails.\n",
        "max_retries: On the model side, you can specify how many times to re-call the API if certain rate-limit or server errors occur.\n",
        "Example with a Retry:"
      ],
      "metadata": {
        "id": "MN-QSnAfBbh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def parse_or_fail(text: str) -> dict:\n",
        "    return json.loads(text)\n",
        "\n",
        "faulty_parser = RunnableLambda(parse_or_fail).with_retry(stop_after_attempt=3)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Here, the parser will attempt up to 3 times if it encounters a JSON parsing error or specific exceptions you define.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hSnUbijDBb5l",
        "outputId": "6a2d981e-c080-41dd-8bef-33f2a80211e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nHere, the parser will attempt up to 3 times if it encounters a JSON parsing error or specific exceptions you define.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.2. Checking for Invalid Tool Calls**\n",
        "\n",
        "If you’re using more advanced “tool calling” with OpenAI (or any model that supports function-calling) and the model tries to pass invalid JSON arguments, an InvalidToolCall can arise. You can catch that in your code and adjust accordingly."
      ],
      "metadata": {
        "id": "ec3ALdTBBcXr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s03QMR1wBdE9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Putting It All Together: A Simple Translator App**\n",
        "\n",
        "Let’s integrate these steps into one cohesive mini-application that:\n",
        "\n",
        "\n",
        "Builds a prompt template.\n",
        "Uses messages for instructions.\n",
        "Parses the output as structured JSON with fallback."
      ],
      "metadata": {
        "id": "W7m2_rDzBdYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.output_parsers.fix import OutputFixingParser  # Fixed import path\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "# Initialize the chat model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Step 1: Create the base prompt template\n",
        "system_msg = \"You are a helpful language translator. Always output valid JSON with a 'translation' field.\"\n",
        "user_msg   = \"Translate to German: {text}\"\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_msg),\n",
        "    (\"user\", user_msg)\n",
        "])\n",
        "\n",
        "# Step 2: Define the JSON parser and an OutputFixing fallback\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "# This fallback tries to fix the model output if we fail to parse\n",
        "fallback_parser = OutputFixingParser.from_llm(\n",
        "    parser=json_parser,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Step 3: Define a chain that assembles everything\n",
        "translation_chain = (\n",
        "    prompt_template\n",
        "    | llm  # calls the LLM with the formatted messages\n",
        "    | fallback_parser  # tries to parse or fix the output\n",
        ")\n",
        "\n",
        "# Step 4: Invoke the chain\n",
        "def translate(text: str) -> dict:\n",
        "    # Pass the user input to the chain\n",
        "    return translation_chain.invoke({\"text\": text})\n",
        "\n",
        "# Example usage\n",
        "result = translate(\"Hello, how are you?\")\n",
        "print(\"Parsed result:\", result)\n",
        "# Should output: {'translation': 'Hallo, wie geht es dir?'}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIESbwv-BePD",
        "outputId": "0ad2aeba-8bca-4e7c-bb99-6ca753433ec8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed result: {'translation': 'Hallo, wie geht es dir?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Bonus: Streaming Responses**\n",
        "\n",
        "When using chat models, you can optionally stream tokens to your console or UI. This helps with user experience in real-time.\n",
        "\n",
        "python\n",
        "Copy\n"
      ],
      "metadata": {
        "id": "7LBT4bIFBesr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm.stream([HumanMessage(content=\"Tell me a very long joke.\")]):\n",
        "    print(chunk.content, end=\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSqbF4SnBfE-",
        "outputId": "97dd804c-ce1b-4e2b-e0e1-f0a513734950"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here’s a long joke for you:\n",
            "\n",
            "Once upon a time in a small village, there lived a farmer named Joe. Joe was known for his incredible luck. No matter what he did, things always seemed to work out in his favor. One day, while plowing his field, he stumbled upon a mysterious old lamp. Curious, he picked it up and gave it a rub.\n",
            "\n",
            "To his astonishment, a genie appeared in a puff of smoke. The genie said, \"Thank you for freeing me! I will grant you three wishes, but there’s a catch. Whatever you wish for, your neighbor, Bob, will receive double.\"\n",
            "\n",
            "Joe thought for a moment and said, \"Okay, for my first wish, I want a million dollars!\" The genie snapped his fingers, and suddenly, Joe found himself with a million dollars in his bank account. But just as he was celebrating, he remembered Bob. The genie informed him that Bob now had two million dollars.\n",
            "\n",
            "Feeling a bit envious but still excited, Joe decided to make his second wish. \"I wish for a beautiful mansion!\" The genie granted the wish, and Joe was thrilled to see his new home. But then the genie reminded him that Bob now had two beautiful mansions.\n",
            "\n",
            "Now Joe was feeling a bit frustrated. He thought long and hard about his final wish. He wanted to be clever and outsmart the situation. Finally, he said, \"For my last wish, I want you to give me a good scare!\" \n",
            "\n",
            "The genie looked puzzled but nodded and snapped his fingers. Suddenly, Joe was terrified! He felt his heart racing, and he was shaking all over. But then he remembered Bob. The genie chuckled and said, \"Congratulations, Joe! Bob is now scared to death!\"\n",
            "\n",
            "Joe was shocked. He had unintentionally wished for Bob's demise! As he stood there, feeling guilty, the genie said, \"Don't worry, Joe. You still have your million dollars and your mansion. You can live happily ever after!\"\n",
            "\n",
            "But Joe couldn't shake the feeling of guilt. He decided to go to the village and confess to the townsfolk what had happened. As he shared his story, the villagers were shocked but also amused. They decided to throw a party in Joe's honor, celebrating his newfound wealth and home.\n",
            "\n",
            "At the party, Joe was still feeling uneasy. He approached the village elder, a wise old man named Harold, and said, \"I feel terrible about what happened to Bob. I didn't mean for any of this to happen!\"\n",
            "\n",
            "Harold smiled and said, \"Joe, sometimes life throws us unexpected challenges. But remember, you have the power to make things right. Why not use your wealth to help others in the village?\"\n",
            "\n",
            "Inspired by Harold's words, Joe decided to use his money to improve the village. He built a school, a hospital, and even a community center. The villagers were grateful, and Joe felt a sense of fulfillment.\n",
            "\n",
            "One day, while he was working on the community center, he noticed a familiar face in the crowd. It was Bob! Joe was shocked. He rushed over and said, \"Bob! I thought you were... well, you know.\"\n",
            "\n",
            "Bob laughed and said, \"Oh, that! The genie gave me a second chance. He said I could come back, but I had to learn a lesson about greed. So here I am, and I see you've done amazing things for the village!\"\n",
            "\n",
            "Joe was overjoyed. He and Bob became friends, and together they worked to make the village even better. They laughed about the whole genie incident and realized that sometimes, the best wishes are the ones that come from the heart.\n",
            "\n",
            "And so, Joe and Bob lived happily ever after, proving that friendship and kindness are worth more than any amount of money. \n",
            "\n",
            "The end! \n",
            "\n",
            "And the moral of the story? Always be careful what you wish for, because you might just end up with a good scare!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Conclusion**\n",
        "\n",
        "- Prompts: Start simple; use direct calls for quick tests.\n",
        "- Prompt Templates: Use ChatPromptTemplate to incorporate both system instructions and user inputs dynamically.\n",
        "Messages: Distinguish roles (SystemMessage, HumanMessage, AIMessage), which is key to controlling conversation flow.\n",
        "- Parsing: If you need structured responses, leverage LangChain’s Output - Parsers or tool calling—both are robust solutions to ensure well-formed outputs.\n",
        "- Error Handling: Stay vigilant about malformed output. Retries, - OutputFixingParser, or fallback logic can help maintain reliability.\n",
        "- Streaming: Great for real-time feedback; easy to set up with .stream() on any modern chat model.\n",
        "\n",
        "With these building blocks in place, you can create powerful, production-grade applications that reason and respond in controlled, structured ways. Feel free to explore LangGraph for more advanced orchestration, or LangSmith for logging and evaluation as you iterate on your AI applications!"
      ],
      "metadata": {
        "id": "nhJ8YeKJBfdF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKRisDzGBf3t"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Homework**\n",
        "\n",
        "Read this case history and build a pubmed search tool that gets latest papers and find the best treatment for this patient\n",
        "\n",
        "\n",
        "Patient Name: John DoeDate of Birth: 03/15/1980Date of Visit: 01/21/2025Medical Record Number: 123456789\n",
        "\n",
        "Subjective:\n",
        "The patient is a 44-year-old male with a history of melanoma, initially diagnosed on 08/10/2018, who presents with a relapsed and progressively challenging case of melanoma. The patient has undergone multiple prior treatments, including wide excision, lymph node dissection, immunotherapy (nivolumab), and targeted therapy (BRAF/MEK inhibitors). Despite these interventions, the disease has demonstrated recurrence and progressive metastatic spread, with recent imaging revealing involvement of the lungs and liver.\n",
        "\n",
        "The patient reports worsening symptoms, including persistent pain, fatigue, significant weight loss, and shortness of breath. Symptom burden has significantly impacted quality of life, with the patient expressing concerns about both the physical and emotional toll of the disease. Additionally, there have been challenges in managing treatment-related side effects, including severe fatigue and nausea.\n",
        "\n",
        "Objective:\n",
        "\n",
        "Vital Signs: BP: 128/82, HR: 92, Temp: 98.6°F, RR: 18, O2 saturation: 94% on room air\n",
        "\n",
        "General Appearance: Cachectic and fatigued\n",
        "\n",
        "Skin: Multiple nodules and pigmentation changes noted on the chest and back\n",
        "\n",
        "Lymph Nodes: Palpable axillary lymph nodes, approximately 2-3 cm in size\n",
        "\n",
        "Lungs: Decreased breath sounds in the lower lobes bilaterally\n",
        "\n",
        "Neurological Exam: No focal deficits noted\n",
        "\n",
        "Other Findings: Mild hepatomegaly noted on abdominal palpation\n",
        "\n",
        "Imaging/Diagnostics:\n",
        "Recent imaging studies (PET/CT dated 01/10/2025) revealed new metastatic lesions in the liver and progression of lung lesions. Biopsy from the liver confirmed BRAF mutation-positive melanoma. Labs show elevated LDH (540 U/L), mild anemia (Hb: 10.2 g/dL), and mildly elevated liver enzymes (ALT: 65 U/L, AST: 70 U/L).\n",
        "\n",
        "Assessment:\n",
        "Relapsed metastatic melanoma with progressive disease despite prior interventions. High symptom burden and limited remaining treatment options. The case is further complicated by resistance to therapy and poor performance status (ECOG 2).\n",
        "\n",
        "Plan:\n",
        "\n",
        "Oncology Consultation: Collaboration with the oncology team to reassess treatment options, including clinical trials, second-line therapies, or palliative measures.\n",
        "\n",
        "Targeted Therapy/Immunotherapy: Evaluate potential for additional or alternative agents based on molecular profiling (e.g., PD-1/CTLA-4 inhibitors).\n",
        "\n",
        "Palliative Care: Integration of supportive and palliative care for symptom management and quality-of-life improvement, addressing pain, fatigue, and psychosocial support needs.\n",
        "\n",
        "Symptom Management: Adjust medications for pain (morphine extended-release 15 mg twice daily), and nausea (ondansetron 8 mg as needed). Consider referral to pain management or other supportive services.\n",
        "\n",
        "Patient and Family Support: Engage patient and family in shared decision-making, provide resources for counseling, and discuss goals of care, including advance care planning.\n",
        "\n",
        "Follow-Up: Close monitoring with follow-up appointment in 2 weeks. Labs and imaging to be repeated in 4 weeks to assess disease trajectory.\n",
        "\n",
        "Provider’s Name: Dr. Jane Smith, MDSignature:Date: 01/21/2025\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "utOEDE7sBgKu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3q0El_CnBg7z"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}