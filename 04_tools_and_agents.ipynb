{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG98S/qviI+cKf1XmGIyZ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IyadSultan/low-coding-AI/blob/main/04_tools_and_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04 Tools and Agents\n",
        "\n",
        "A very important link https://python.langchain.com/docs/integrations/tools/\n",
        "\n",
        "Below is a step-by-step tutorial demonstrating how to combine OpenAI Function Calling and the LangChain ecosystem for building “tools and agents.” This tutorial focuses first on the foundations—how to call functions, parse outputs, and chain calls. It then culminates in a more advanced example: reading a medical note, summarizing it, finding the best PubMed search results, retrieving references, and constructing a recommendation letter.\n",
        "All code blocks are designed to run in Google Colab (or similar Python environments). Some blocks are examples to illustrate concepts (you may not need to run them all, but they’re helpful references). If you plan to save your results, be aware that Colab resets often, so consider downloading your notebook or saving to GitHub."
      ],
      "metadata": {
        "id": "3cqdUS6qATvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Environment Setup**"
      ],
      "metadata": {
        "id": "8u-329PrBJma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "uDthrjp3AUee"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Quick Start with OpenAI Function Calling**\n",
        "\n",
        "Let’s begin by showing the essential structure for function calling with the OpenAI ChatCompletion endpoints.\n",
        "\n"
      ],
      "metadata": {
        "id": "kcMdaasVAVhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BpKRhnsFAVbs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Introducing LangChain’s “Runnable” and “Chain” Utilities**\n",
        "\n",
        "3.1 Simple “Runnable” Pipeline\n",
        "LangChain 0.0.305+ introduced the Runnable classes, which let you chain various pieces (prompts, LLMs, output parsers) in a pipeline style.\n"
      ],
      "metadata": {
        "id": "GXlivhSIAVXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "-EYpzTt4l4ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "# from langchain_core.output_parsers import StrOutputParser  # Alternative import if needed\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Build a simple chain: Prompt -> LLM -> OutputParser\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)  # Specify model name\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Create and run the chain\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "try:\n",
        "    result = chain.invoke({\"topic\": \"bears\"})\n",
        "    print(result)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFareswTAVR4",
        "outputId": "bb2726f3-3717-4e01-8cf4-937f97107adc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do bears have hairy coats?\n",
            "\n",
            "Because they look silly in sweaters!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple chain: Prompt -> LLM -> OutputParser\n",
        "prompt = ChatPromptTemplate.from_template(\"How is the weather in {city}\")\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)  # Specify model name\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Create and run the chain\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "result = chain.invoke({\"city\": \"Amman\"})\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72Nye3SuDJzh",
        "outputId": "4dcc61ed-3350-4977-e29d-85ca94e495da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't have real-time data access to provide current weather updates. To find the latest weather information for Amman, I recommend checking a reliable weather website or using a weather app.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weather_dic={\n",
        "\"Amman\": \"Mild spring weather, around 20°C (68°F), partly cloudy\",\n",
        "\"Amsterdam\": \"Cool spring conditions, around 12°C (54°F), likely overcast with chance of rain\",\n",
        "\"Auckland\": \"Autumn season, mild temperatures around 18°C (64°F), partly cloudy with occasional showers\",\n",
        "\"Athens\": \"Pleasant spring weather, around 22°C (72°F), mostly sunny\",\n",
        "\"Adelaide\": \"Autumn weather, around 21°C (70°F), clear skies\"\n",
        "}"
      ],
      "metadata": {
        "id": "epZlT0kaN43V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a function that uses this dictionary to return the weather in any specified city\n",
        "\n",
        "\n",
        "def get_weather(city):\n",
        "  \"\"\"\n",
        "  Returns the weather in the specified city using the weather_dic dictionary.\n",
        "\n",
        "  Args:\n",
        "      city: The name of the city.\n",
        "\n",
        "  Returns:\n",
        "      The weather description for the city, or None if the city is not found.\n",
        "  \"\"\"\n",
        "  return weather_dic.get(city)"
      ],
      "metadata": {
        "id": "nkK8BQkqN4ps"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weather(\"Amman\")"
      ],
      "metadata": {
        "id": "1unfgRVBN4L6",
        "outputId": "1dda4cfc-3950-4e69-be0e-0eec41e8e2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mild spring weather, around 20°C (68°F), partly cloudy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "get_weather_json_format={\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Get the current weather for a given city.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"city\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The name of the city to get weather for.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"city\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "# Define the function as a tool\n",
        "functions = [get_weather_json_format]\n",
        "\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "\n",
        "def chat_with_gpt(user_message):\n",
        "    \"\"\"\n",
        "    Sends a message to the GPT model and allows it to use the get_weather function when needed.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",  # Correct model name\n",
        "        messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "        functions=functions,\n",
        "        function_call=\"auto\"  # Enable function calling\n",
        "    )\n",
        "\n",
        "\n",
        "    message = response.choices[0].message\n",
        "\n",
        "\n",
        "    # Handle function calls if present\n",
        "    if message.function_call:\n",
        "        function_name = message.function_call.name\n",
        "        function_args = json.loads(message.function_call.arguments)\n",
        "\n",
        "        if function_name == \"get_weather\":\n",
        "            function_response = get_weather(function_args[\"city\"])\n",
        "\n",
        "            # Send the function response back to GPT\n",
        "            final_response = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": user_message},\n",
        "                    message,\n",
        "                    {\n",
        "                        \"role\": \"function\",\n",
        "                        \"name\": \"get_weather\",\n",
        "                        \"content\": function_response\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "            return final_response.choices[0].message.content\n",
        "\n",
        "    return message.content\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "user_input = \"What's the weather like in Amman today?\"\n",
        "print(chat_with_gpt(user_input))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cENiFHAKO5Rk",
        "outputId": "8741720d-3062-4c4a-be56-e30687a40a2c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Amman today is mild spring weather, with temperatures around 20°C (68°F) and partly cloudy skies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tip(x):\n",
        "  return x*0.15\n",
        "\n"
      ],
      "metadata": {
        "id": "Bgop4VTGrvP8",
        "outputId": "2cb06b6a-172b-4cee-f6bd-ae5d79c2e9a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make a pydanti model that use the tip funciton\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class TipCalculator(BaseModel):\n",
        "    amount: float = Field(..., description=\"The amount to calculate the tip on.\")\n",
        "\n",
        "    def tip(self):\n",
        "      return self.amount * 0.15"
      ],
      "metadata": {
        "id": "uQwjgyP5sHO1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TipCalculator(amount=100).tip()"
      ],
      "metadata": {
        "id": "wbnmq-aXsO-f",
        "outputId": "a779adb0-e2e1-4564-e62d-d18e96cc874a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from pprint import pprint\n",
        "\n",
        "tips=convert_pydantic_to_openai_function(TipCalculator)\n",
        "# pprint(tips)\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "user_message = \"Our lunch bill was 80 JD, how much should I tip the waiter?\"\n",
        "\n",
        "response=client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "    ],\n",
        "    functions=[tips],\n",
        "    function_call=\"auto\"\n",
        ")\n",
        "\n",
        "pprint(response)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "dAZRJw_ksWzs",
        "outputId": "a9e2d8f8-6bf2-49cc-c1fc-87257efe5ebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-AvSkGxlciRHyRxYWXAgeyqsWk7AAw', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=FunctionCall(arguments='{\"amount\":80}', name='TipCalculator'), tool_calls=None))], created=1738258556, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_72ed7ab54c', usage=CompletionUsage(completion_tokens=16, prompt_tokens=62, total_tokens=78, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: extract function_call from this if it exists and execute the function using the arguement\n",
        "# ChatCompletion(id='chatcmpl-AvSkGxlciRHyRxYWXAgeyqsWk7AAw', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=FunctionCall(arguments='{\"amount\":80}', name='TipCalculator'), tool_calls=None))], created=1738258556, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_72ed7ab54c', usage=CompletionUsage(completion_tokens=16, prompt_tokens=62, total_tokens=78, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
        "\n",
        "import json\n",
        "\n",
        "# Assuming 'response' is defined as in the provided code\n",
        "function_call = response.choices[0].message.function_call\n",
        "\n",
        "if function_call:\n",
        "    function_name = function_call.name\n",
        "    arguments = json.loads(function_call.arguments)\n",
        "\n",
        "    if function_name == \"TipCalculator\":\n",
        "        tip_calculator = TipCalculator(**arguments)\n",
        "        tip_amount = tip_calculator.tip()\n",
        "        print(f\"The tip amount is: {tip_amount}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RpByhleNSYx0",
        "outputId": "0809c421-c55e-45f4-cfdb-2f6de4449cd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tip amount is: 12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yDDaYHl6AVNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "\n",
        "# Create a mock weather database\n",
        "weather_db = {\n",
        "    \"SFO\": {\n",
        "        \"city\": \"San Francisco\",\n",
        "        \"temperature\": 65,\n",
        "        \"conditions\": \"Foggy\",\n",
        "        \"humidity\": 75,\n",
        "        \"wind_speed\": 12,\n",
        "        \"last_updated\": \"2025-01-29 08:00:00\"\n",
        "    },\n",
        "    \"JFK\": {\n",
        "        \"city\": \"New York\",\n",
        "        \"temperature\": 45,\n",
        "        \"conditions\": \"Partly Cloudy\",\n",
        "        \"humidity\": 60,\n",
        "        \"wind_speed\": 15,\n",
        "        \"last_updated\": \"2025-01-29 08:00:00\"\n",
        "    },\n",
        "    \"LAX\": {\n",
        "        \"city\": \"Los Angeles\",\n",
        "        \"temperature\": 75,\n",
        "        \"conditions\": \"Sunny\",\n",
        "        \"humidity\": 50,\n",
        "        \"wind_speed\": 8,\n",
        "        \"last_updated\": \"2025-01-29 08:00:00\"\n",
        "    },\n",
        "    \"ORD\": {\n",
        "        \"city\": \"Chicago\",\n",
        "        \"temperature\": 32,\n",
        "        \"conditions\": \"Snow\",\n",
        "        \"humidity\": 80,\n",
        "        \"wind_speed\": 20,\n",
        "        \"last_updated\": \"2025-01-29 08:00:00\"\n",
        "    },\n",
        "    \"AMM\": {\n",
        "        \"city\": \"Amman\",\n",
        "        \"temperature\": 82,\n",
        "        \"conditions\": \"Thunderstorms\",\n",
        "        \"humidity\": 85,\n",
        "        \"wind_speed\": 18,\n",
        "        \"last_updated\": \"2025-01-29 08:00:00\"\n",
        "    }\n",
        "}\n",
        "\n",
        "class WeatherSearch(BaseModel):\n",
        "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
        "    airport_code: str = Field(description=\"airport code to get weather for\")\n",
        "\n",
        "    def get_weather(self) -> Optional[dict]:\n",
        "        \"\"\"Get weather for the specified airport\"\"\"\n",
        "        try:\n",
        "            if self.airport_code not in weather_db:\n",
        "                return {\n",
        "                    \"error\": f\"No weather data available for {self.airport_code}\",\n",
        "                    \"available_airports\": list(weather_db.keys())\n",
        "                }\n",
        "\n",
        "            weather_data = weather_db[self.airport_code]\n",
        "            return {\n",
        "                \"airport\": self.airport_code,\n",
        "                \"city\": weather_data[\"city\"],\n",
        "                \"temperature\": f\"{weather_data['temperature']}°F\",\n",
        "                \"conditions\": weather_data[\"conditions\"],\n",
        "                \"humidity\": f\"{weather_data['humidity']}%\",\n",
        "                \"wind_speed\": f\"{weather_data['wind_speed']} mph\",\n",
        "                \"last_updated\": weather_data[\"last_updated\"]\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "weather_function = convert_pydantic_to_openai_function(WeatherSearch)\n",
        "\n",
        "# Test the function with different airports\n",
        "test_codes = [\"QAA\", \"SFO\", \"JFK\", \"LAX\", \"ORD\", \"DFW\"]  # DFW isn't in our database\n",
        "\n",
        "for code in test_codes:\n",
        "    try:\n",
        "        search = WeatherSearch(airport_code=code)\n",
        "        weather_data = search.get_weather()\n",
        "        print(f\"\\nWeather lookup for {code}:\")\n",
        "        print(weather_data)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError processing {code}: {e}\")\n",
        "\n",
        "# Print the function definition\n",
        "print(\"\\nFunction definition:\")\n",
        "print(weather_function)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGkHA-6xAVHH",
        "outputId": "0c0f0876-6872-40a4-ac4c-58a5659a7c36"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Weather lookup for QAA:\n",
            "{'error': 'No weather data available for QAA', 'available_airports': ['SFO', 'JFK', 'LAX', 'ORD', 'AMM']}\n",
            "\n",
            "Weather lookup for SFO:\n",
            "{'airport': 'SFO', 'city': 'San Francisco', 'temperature': '65°F', 'conditions': 'Foggy', 'humidity': '75%', 'wind_speed': '12 mph', 'last_updated': '2025-01-29 08:00:00'}\n",
            "\n",
            "Weather lookup for JFK:\n",
            "{'airport': 'JFK', 'city': 'New York', 'temperature': '45°F', 'conditions': 'Partly Cloudy', 'humidity': '60%', 'wind_speed': '15 mph', 'last_updated': '2025-01-29 08:00:00'}\n",
            "\n",
            "Weather lookup for LAX:\n",
            "{'airport': 'LAX', 'city': 'Los Angeles', 'temperature': '75°F', 'conditions': 'Sunny', 'humidity': '50%', 'wind_speed': '8 mph', 'last_updated': '2025-01-29 08:00:00'}\n",
            "\n",
            "Weather lookup for ORD:\n",
            "{'airport': 'ORD', 'city': 'Chicago', 'temperature': '32°F', 'conditions': 'Snow', 'humidity': '80%', 'wind_speed': '20 mph', 'last_updated': '2025-01-29 08:00:00'}\n",
            "\n",
            "Weather lookup for DFW:\n",
            "{'error': 'No weather data available for DFW', 'available_airports': ['SFO', 'JFK', 'LAX', 'ORD', 'AMM']}\n",
            "\n",
            "Function definition:\n",
            "{'name': 'WeatherSearch', 'description': 'Call this with an airport code to get the weather at that airport', 'parameters': {'properties': {'airport_code': {'description': 'airport code to get weather for', 'type': 'string'}}, 'required': ['airport_code'], 'type': 'object'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(weather_function)"
      ],
      "metadata": {
        "id": "5_suIDNvQYvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Binding Functions to a Chat Model**"
      ],
      "metadata": {
        "id": "gDqT70jbAVBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5, api_key=OPENAI_API_KEY).bind(functions=[weather_function])\n",
        "\n",
        "def process_weather_query(query):\n",
        "    response = model.invoke(query)\n",
        "    if 'function_call' in response.additional_kwargs:\n",
        "        args = json.loads(response.additional_kwargs['function_call']['arguments'])\n",
        "        return WeatherSearch(**args).get_weather()\n",
        "    return \"Weather data unavailable\"\n",
        "\n",
        "# Create prompt template and chain once\n",
        "prompt = ChatPromptTemplate([(\"human\", \"Nicely describe the weather in this airport and wish a nice trip using this data: {weather_data}\")])\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# Process queries\n",
        "for query in [\"What's the weather in SFO?\", \"Weather at AMMAN\", \"What is the weather in CCC\"]:\n",
        "    weather_data = process_weather_query(query)\n",
        "    # print(f\"\\nQuery: {query}\")\n",
        "    # print(f\"Weather data: {weather_data}\")\n",
        "    # Pass the weather data with the correct variable name\n",
        "    response = chain.invoke({\"weather_data\": weather_data})\n",
        "    print(f\"Description: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OZ1oF88AU6F",
        "outputId": "409c80c6-7310-4cbd-ab9e-ab19c687439a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description: The weather at San Francisco International Airport (SFO) is currently a cool 65°F, enveloped in a gentle fog that adds a touch of mystique to the surroundings. With a humidity level of 75%, the air feels refreshingly crisp, complemented by a light breeze blowing at 12 mph. \n",
            "\n",
            "As you prepare for your journey, embrace the enchanting atmosphere of San Francisco. Wishing you a wonderful trip filled with delightful experiences and safe travels!\n",
            "Description: The weather at Queen Alia International Airport (AMM) in Amman is currently quite dynamic, with a temperature of 82°F. Thunderstorms are rolling through the area, creating a dramatic backdrop for your travels. The humidity is elevated at 85%, adding a touch of warmth to the air, while a brisk wind is blowing at 18 mph, offering a refreshing contrast.\n",
            "\n",
            "As you prepare for your journey, please stay safe and keep an eye on the weather updates. Wishing you a wonderful trip filled with memorable experiences!\n",
            "Description: It seems that there is no weather data available for the airport with the code CCC. However, if you're traveling to or from any of the following airports—San Francisco International Airport (SFO), John F. Kennedy International Airport (JFK), Los Angeles International Airport (LAX), O'Hare International Airport (ORD), or Queen Alia International Airport (AMM)—I would be happy to provide you with a weather update for those locations.\n",
            "\n",
            "Regardless of your destination, I wish you a wonderful trip filled with exciting adventures and safe travels!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3 Force-Calling a Function**\n",
        "You can also force the model to always call a particular function:\n"
      ],
      "metadata": {
        "id": "lG30UwSfAUu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_function = model.bind(\n",
        "    functions=[weather_function],\n",
        "    function_call={\"name\": \"WeatherSearch\"}  # Force the model to always use this function\n",
        ")\n",
        "output_parser = StrOutputParser()\n",
        "chain=model_with_function|output_parser\n",
        "# print(model_with_function.invoke(\"What's the weather in SFO?\"))\n",
        "chain.invoke(\"What's the weather in SFO?\")\n"
      ],
      "metadata": {
        "id": "FeSt0gK1ATqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Tools and Agents in LangChain**\n",
        "\n",
        "In LangChain, “tools” are simply Python callables (functions) that can be exposed to an LLM. Agents decide which tool to call and when to call it.\n",
        "5.1 Defining Tools\n",
        "LangChain supplies a @tool decorator for convenience. Example:\n",
        "from langchain.agents import tool\n"
      ],
      "metadata": {
        "id": "Y_v4g5Q7ATlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install Wikipedia\n",
        "from langchain.agents import tool\n",
        "import wikipedia\n",
        "\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    \"\"\"Search Wikipedia and get page summaries.\"\"\"\n",
        "    try:\n",
        "        # Get search results\n",
        "        page = wikipedia.page(query, auto_suggest=False)\n",
        "        return f\"Title: {page.title}\\nSummary: {page.summary}\"\n",
        "    except:\n",
        "        try:\n",
        "            # If direct search fails, try getting the first result from search\n",
        "            results = wikipedia.search(query, results=1)\n",
        "            if results:\n",
        "                page = wikipedia.page(results[0], auto_suggest=False)\n",
        "                return f\"Title: {page.title}\\nSummary: {page.summary}\"\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return \"No Wikipedia article found.\"\n",
        "\n",
        "print(search_wikipedia(\"Python programming\"))\n"
      ],
      "metadata": {
        "id": "LGANDA8mAThf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.2 Converting Tools to OpenAI Functions**"
      ],
      "metadata": {
        "id": "us0BtMaSATd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "\n",
        "formatted_tool = format_tool_to_openai_function(search_wikipedia)\n",
        "print(formatted_tool)\n"
      ],
      "metadata": {
        "id": "gqSFWHX0ATYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.3 A Simple Routing Example**\n",
        "If you pass multiple tools, the LLM will pick which function to call or produce a direct textual response. For instance:\n",
        "\n"
      ],
      "metadata": {
        "id": "iMQCFU6HATR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
        "\n",
        "\n",
        "functions = [formatted_tool, weather_function]  # Suppose we have two\n",
        "\n",
        "\n",
        "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
        "chain=model|output_parser\n",
        "chain.invoke(\"What's the weather in SFO?\")\n",
        "# result = model.invoke(\"What is the weather in Boston right now?\")\n",
        "# print(result)"
      ],
      "metadata": {
        "id": "Tf6OM8HHATL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install xmltodict"
      ],
      "metadata": {
        "id": "NiwPHM2fTv1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.pubmed.tool import PubmedQueryRun\n",
        "\n",
        "# Initialize the PubMed query tool\n",
        "pubmed_tool = PubmedQueryRun()\n",
        "\n",
        "# Define the search query for lung cancer\n",
        "query = \"lung cancer\"\n",
        "\n",
        "# Run the query to get results\n",
        "results = pubmed_tool.invoke(query)\n",
        "\n",
        "# Extract and display 10 abstracts from the results\n",
        "print(results)"
      ],
      "metadata": {
        "id": "lowd_KtETvkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import PubMedRetriever\n",
        "retriever = PubMedRetriever()\n",
        "result=retriever.invoke('Lung Cancer')\n",
        "print(len(result))\n",
        "# print(result[0])\n"
      ],
      "metadata": {
        "id": "WxSC8P_MTvKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_community\n",
        "\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "\n",
        "search = GoogleSearchAPIWrapper()\n",
        "\n",
        "tool = Tool(\n",
        "    name=\"google_search\",\n",
        "    description=\"Search Google for recent results.\",\n",
        "    func=search.run,\n",
        ")"
      ],
      "metadata": {
        "id": "JdXmYY7dTvBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lxywz5XXTuvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Building a Conversational Agent with Memory**\n",
        "Let’s combine:\n",
        "Tools for weather, Wikipedia.\n",
        "A Chat model that can call them as needed.\n",
        "Conversation memory so the agent can remember earlier turns.\n"
      ],
      "metadata": {
        "id": "U97j2AT7ATHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain_community\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.agents import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "import wikipedia\n",
        "\n",
        "\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    \"\"\"Search Wikipedia and get page summaries.\"\"\"\n",
        "    try:\n",
        "        page = wikipedia.page(query, auto_suggest=False)\n",
        "        return f\"Title: {page.title}\\nSummary: {page.summary}\"\n",
        "    except:\n",
        "        try:\n",
        "            results = wikipedia.search(query, results=1)\n",
        "            if results:\n",
        "                page = wikipedia.page(results[0], auto_suggest=False)\n",
        "                return f\"Title: {page.title}\\nSummary: {page.summary}\"\n",
        "        except:\n",
        "            pass\n",
        "    return \"No Wikipedia article found.\"\n",
        "\n",
        "@tool\n",
        "def get_current_temperature(location: str) -> str:\n",
        "    \"\"\"Get the current temperature for a location.\"\"\"\n",
        "    return f\"It is 70F in {location}\"\n",
        "\n",
        "# Helper function to format intermediate steps\n",
        "def format_to_openai_functions(intermediate_steps):\n",
        "    \"\"\"Format intermediate steps to OpenAI function messages.\"\"\"\n",
        "    messages = []\n",
        "    for action, observation in intermediate_steps:\n",
        "        messages.append({\n",
        "            \"tool\": action.tool,\n",
        "            \"tool_input\": action.tool_input,\n",
        "            \"observation\": observation\n",
        "        })\n",
        "    return messages\n",
        "\n",
        "# Format tools\n",
        "tools = [search_wikipedia, get_current_temperature]\n",
        "functions = [format_tool_to_openai_function(t) for t in tools]\n",
        "\n",
        "# Initialize model with functions\n",
        "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
        "\n",
        "# Create prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Remember information about the user.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "# Create the chain with initial empty intermediate steps\n",
        "chain = RunnablePassthrough.assign(\n",
        "    chat_history=lambda x: x.get(\"chat_history\", []),\n",
        "    intermediate_steps=lambda x: x.get(\"intermediate_steps\", []),\n",
        "    agent_scratchpad=lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
        ") | prompt | model | OpenAIFunctionsAgentOutputParser()\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
        "\n",
        "# Create agent executor\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=chain,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Test the agent\n",
        "try:\n",
        "    # First interaction\n",
        "    response1 = agent_executor.invoke({\"input\": \"My name is Bob.\"})\n",
        "    print(\"Response 1:\", response1)\n",
        "\n",
        "    # Second interaction\n",
        "    response2 = agent_executor.invoke({\"input\": \"What's my name?\"})\n",
        "    print(\"Response 2:\", response2)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "1dlo2l_tATCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "feUFMxOdAS88"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nFruIB7hAS4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zbOSjGc-ASzt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7lMpWJFcASuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QV-c18yIASo5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuRW1dCCASkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h8Ux2BwUASe3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qXP2oaMqASZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y7uGMzcYASTc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KMCLuPIDASNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FyBaTJGHASHw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HR6LkfEqASBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pmNwXUt7AR8I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erK1VHByAR1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_dMjK9FuARu_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-X7gPKAZARo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kVqblyorARiw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FebotAV0ARb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R9tTd5BSARVA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yuea4ju_AROO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1ZZL-FbnARH7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NlqCwKIVARBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZgrpBoA2AQ7g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJMr61ZDAQ0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "88gFjgcWAQt-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuM4zeNfAQm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "78vl6O7kAQhV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fWaX02h-AQZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k6yEextnAQTz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12MbXHloAQNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9mzRIOpkAQG2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rK_TFACAAP_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pC-Sg9HUAP44"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kc6AqwYNAPxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "htYg7vPKAPps"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LPPaXP1APjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QqvcCSOcAPbn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_a2slBuAPVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gFCjIpJpAPNo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxK_Eak6APGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZOkzLl_nAO-4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ik1rZXZ6AO4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V5hDmSRzAOw7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BY-Cy6mBAOmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3mhyD9mJAOfu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1OZJdTDWAOYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynAyNA5xAOQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}